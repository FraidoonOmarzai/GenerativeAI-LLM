{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3jjli9bGIL_S",
        "V_vTKVBOIN5E",
        "z973XXJNIj0u",
        "ZLsDak1bJYIY"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PaLM 2 Project using LangChain"
      ],
      "metadata": {
        "id": "ujVLqYlBIF9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "3jjli9bGIL_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af-3qZBnHOaS",
        "outputId": "d77a00d8-1b21-420d-e309-33a95f0375aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain)\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.48 (from langchain)\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.48->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.5 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langsmith-0.1.54 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-4.0.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Installing collected packages: pinecone-client\n",
            "Successfully installed pinecone-client-4.0.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install pypdf\n",
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "V_vTKVBOIN5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Pinecone as PineconeLang\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pinecone import Pinecone as PineconeClient\n",
        "\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "ok0nFL2QIUmm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the pdf file and extract the text\n",
        "- i just loaded my cv to pdf dir"
      ],
      "metadata": {
        "id": "z973XXJNIj0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdf"
      ],
      "metadata": {
        "id": "oISZu4PzIjCN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdf\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "OdBvikhcIv7R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUcZYQAMJP1Y",
        "outputId": "850a1028-7984-4597-f3e6-28f3c62f5787"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Fraidoon\\nOmarzai\\nBirmingham,\\nUK\\n|\\nfraidoonomarzai99@gmail.com\\n|\\n+44\\n(0)\\n7879751613\\nlinkedin.com/in/fraidoonomarzai\\n|\\ngithub.com/FraidoonOmarzai\\nSUMMAR Y\\nHighly\\nmotivated\\nand\\nanalytical\\nAI\\nand\\nMachine\\nLearning\\nfresher,\\ndedicated\\nto\\nacquiring\\nknowledge\\nin\\nemerging\\nareas\\nof\\ntechnology\\nand\\nbusiness,\\nand\\nactively\\nengaged\\nin\\npractical\\nMachine\\nLearning\\nand\\nDeep\\nLearning\\nprojects.\\nEDUCA TION\\nMSc\\nin\\nArtificial\\nIntelligence\\n(MSc\\nAI)\\n-\\nComputer\\nScience\\nSep\\n2023\\n-\\nPresent\\nAston\\nUniversity ,\\nBirmingham\\n(UK)\\nBachelor\\nof\\nComputer\\nApplication\\n(B.C.A.)\\n-\\nComputer\\nScience\\nJuly\\n2019\\n-\\nOct\\n2022\\nBangalore\\nUniversity ,\\nBangalore\\n(India)\\nACADEMIC\\nPROJECTS\\nEnd-T o-End-MLOps\\n|\\nStroke\\nDisease\\nGitHub-Link\\n●\\nI\\nImplemented\\na\\ncomplete\\nMachine\\nLearning\\nProject\\nlifecycle\\n(acquiring\\ndata,\\npreprocessing,\\nmodel\\ntraining,\\nand\\nAWS\\ndeployment).\\n●\\nI\\nutilized\\nthe\\nlatest\\ntechnologies\\nassociated\\nwith\\nend-to-end\\nMLOps\\nprojects\\nlike\\nDVC,\\nMLﬂow,\\nDocker,\\nGitHub\\nActions\\n(CI/CD),\\nand\\nAmazon\\nWeb\\nServices\\n(AWS).\\nRSNA-Pneumonia-Detection\\nGitHub-Link\\n●\\nI\\nExecuted\\na\\ncomplete\\nDeep\\nLearning\\nProject\\nlife-cycle\\nwhile\\nleveraging\\nMLOps\\ntechnologies\\nand\\nincorporating\\nTransfer\\nLearning.\\n●\\nUsed\\nTools:\\nGitHub,\\nTensorFlow,\\nDagshub,\\nDVC,\\nDocker,\\nGitHub-Action,\\nand\\nAWS.\\nAI\\nIn\\nRadiology\\nGitHub-Link\\n●\\nProjects:\\nPneumonia\\nDetection,\\nCardiac\\nDetection.\\n●\\nThe\\nprimary\\ngoal\\nwas\\ndeveloping\\nDeep\\nLearning\\narchitectures\\nfor\\nmedical\\nimage\\nanalysis,\\nencompassing\\nthe\\nfollowing\\nsteps:\\nacquiring\\nthe\\ndataset,\\npreprocessing\\nthe\\ndataset,\\nand\\ntraining\\nthe\\nmodel.\\n●\\nUsed\\nTools\\n:\\nTensorFlow,\\nPy\\nTorch,\\nPy\\nTorch-Lighting,\\nNIFTI,\\nPyDicom,\\nCV2.', metadata={'source': 'pdf/CV_Fraidoon.pdf', 'page': 0}),\n",
              " Document(page_content='WORK\\nEXPERIENCE\\nMachine\\nLearning\\nIntern\\nFeb\\n2023\\n-\\nMar\\n2023\\nSuvidha\\nFoundation\\n|\\nBangalore,\\nIndia\\n●\\nReading\\nresearch\\npapers\\nand\\nimplementing\\nthem.\\n●\\nThe\\ndomain\\nwas\\nMachine\\nLearning\\nand\\nnatural\\nlanguage\\nprocessing\\nMachine\\nLearning\\nIntern\\nAug\\n2022\\n-\\nOct\\n2022\\nFeyNN\\nLabs\\n|\\nBangalore,\\nIndia\\n●\\nThe\\nfocus\\nwas\\nusing\\nMachine\\nLearning\\nin\\nMarket\\nSegmentation\\nand\\nAnalysis.\\n●\\nProject-1\\n(Solo):\\nAI\\nProduct/Service\\nPrototype\\nIdeation.\\n●\\nProject-2\\n(Team):\\nMarket\\nSegmentation\\nusing\\nData\\nAnalysis\\nand\\nMachine\\nLearning.\\n●\\nProject-3\\n(Team):\\nAI\\nProduct/Service\\nPrototype\\nDevelopment.\\nSKILLS\\nMachine\\nLearning\\n(ML)\\n|\\nDeep\\nLearning\\n(DL)\\n|\\nComputer\\nVision\\n(CV)\\n|\\nNatural\\nLanguage\\nProcessing\\n(NLP)\\n|\\nMLOps\\n|\\nTensorFlow\\n|\\nPy\\nTorch\\n|\\nPython\\n|\\nFlask\\n|\\nSQL\\n|\\nGitHub\\nActions\\n(CI/CD)\\n|\\nAWS\\n|\\nDocker\\n|\\nDVC\\n|\\nDagshub\\n|\\nJavaScript\\n|\\nLinux\\n|\\nMonitoring\\n|\\nCommunication\\n|\\nLeadership\\n|\\nTime\\nManagement\\n|\\nAdaptability\\n|\\nProblem\\nSolving\\n|\\nTeamwork\\n|\\nCreativity', metadata={'source': 'pdf/CV_Fraidoon.pdf', 'page': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### split the data into chunks"
      ],
      "metadata": {
        "id": "ZLsDak1bJYIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                chunk_overlap=20)\n",
        "\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "PwwgyypMJRWd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i46aCricJpYy",
        "outputId": "99fda8bc-f1cd-46c2-b486-7003a4572346"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the embedding"
      ],
      "metadata": {
        "id": "eo1xE8CSJ2xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "HmRmVlUbJ8NE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=GooglePalmEmbeddings()"
      ],
      "metadata": {
        "id": "lXxyIPSAJr1S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbSGmP3BJ6V2",
        "outputId": "f7badccc-7ea7-4b3a-f744-8b090111e82b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GooglePalmEmbeddings(client=<module 'google.generativeai' from '/usr/local/lib/python3.10/dist-packages/google/generativeai/__init__.py'>, google_api_key=None, model_name='models/embedding-gecko-001', show_progress_bar=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(\"How are you\")\n",
        "len(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d3F_WUJJKSzE",
        "outputId": "0ea69519-d4b3-475a-d33e-e6ddf2eb1eb1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone section"
      ],
      "metadata": {
        "id": "65h54QYOKfEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "\n",
        "cloud = userdata.get('PINECONE_CLOUD')\n",
        "region = userdata.get('PINECONE_REGION')"
      ],
      "metadata": {
        "id": "IqkqZ2SYKm_i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = PineconeClient(api_key=PINECONE_API_KEY)\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ],
      "metadata": {
        "id": "sBq1djzIKu3I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### create the index"
      ],
      "metadata": {
        "id": "zQqU4TL-KiNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'palm'"
      ],
      "metadata": {
        "id": "lSty0i3WKaP1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=768,  # dimensionality of text-embedding-ada-002\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-z2Lnn-K2Ys",
        "outputId": "7da3e419-bae0-43f3-8be1-f9ee92c70969"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 768,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### create embedding for each chunks and Store the data in pinecone"
      ],
      "metadata": {
        "id": "f_6llqoKLSBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore=PineconeLang.from_texts([t.page_content for t in text_chunks],\n",
        "                                    embeddings,\n",
        "                                    index_name=index_name)"
      ],
      "metadata": {
        "id": "eHZJcIIKK6qx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPJ66oOALZwP",
        "outputId": "fdabb7d5-4494-417c-99f1-fc76d62a3d3c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_community.vectorstores.pinecone.Pinecone object at 0x7cdd690d9bd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### load the index if already we have it"
      ],
      "metadata": {
        "id": "e9rRM2osL8QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeLang.from_existing_index(index_name, embeddings)\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeAIe8jMMAyy",
        "outputId": "57b5544f-df7a-4442-e248-4f6ba5fe5ead"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.pinecone.Pinecone at 0x7cdd68fdd3c0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Similarity Search"
      ],
      "metadata": {
        "id": "V2ERIxm3MnGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"whick skills\""
      ],
      "metadata": {
        "id": "i6FfqPFvMjJ4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectorstore.similarity_search(query, k=3)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "ussh_IIvMtCl",
        "outputId": "86a3c2d1-0557-45a7-bfa0-2fcf21e61bcf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='and\\nMachine\\nLearning.\\n●\\nProject-3\\n(Team):\\nAI\\nProduct/Service\\nPrototype\\nDevelopment.\\nSKILLS\\nMachine\\nLearning\\n(ML)\\n|\\nDeep\\nLearning\\n(DL)\\n|\\nComputer\\nVision\\n(CV)\\n|\\nNatural\\nLanguage\\nProcessing\\n(NLP)\\n|\\nMLOps\\n|\\nTensorFlow\\n|\\nPy\\nTorch\\n|\\nPython\\n|\\nFlask\\n|\\nSQL\\n|\\nGitHub\\nActions\\n(CI/CD)\\n|\\nAWS\\n|\\nDocker\\n|\\nDVC\\n|\\nDagshub\\n|\\nJavaScript\\n|\\nLinux\\n|\\nMonitoring\\n|\\nCommunication\\n|\\nLeadership\\n|\\nTime\\nManagement\\n|\\nAdaptability\\n|\\nProblem\\nSolving\\n|\\nTeamwork\\n|\\nCreativity'),\n",
              " Document(page_content='Fraidoon\\nOmarzai\\nBirmingham,\\nUK\\n|\\nfraidoonomarzai99@gmail.com\\n|\\n+44\\n(0)\\n7879751613\\nlinkedin.com/in/fraidoonomarzai\\n|\\ngithub.com/FraidoonOmarzai\\nSUMMAR Y\\nHighly\\nmotivated\\nand\\nanalytical\\nAI\\nand\\nMachine\\nLearning\\nfresher,\\ndedicated\\nto\\nacquiring\\nknowledge\\nin\\nemerging\\nareas\\nof\\ntechnology\\nand\\nbusiness,\\nand\\nactively\\nengaged\\nin\\npractical\\nMachine\\nLearning\\nand\\nDeep\\nLearning\\nprojects.\\nEDUCA TION\\nMSc\\nin\\nArtificial\\nIntelligence\\n(MSc\\nAI)\\n-\\nComputer\\nScience\\nSep\\n2023\\n-\\nPresent\\nAston\\nUniversity ,\\nBirmingham\\n(UK)'),\n",
              " Document(page_content='Web\\nServices\\n(AWS).\\nRSNA-Pneumonia-Detection\\nGitHub-Link\\n●\\nI\\nExecuted\\na\\ncomplete\\nDeep\\nLearning\\nProject\\nlife-cycle\\nwhile\\nleveraging\\nMLOps\\ntechnologies\\nand\\nincorporating\\nTransfer\\nLearning.\\n●\\nUsed\\nTools:\\nGitHub,\\nTensorFlow,\\nDagshub,\\nDVC,\\nDocker,\\nGitHub-Action,\\nand\\nAWS.\\nAI\\nIn\\nRadiology\\nGitHub-Link\\n●\\nProjects:\\nPneumonia\\nDetection,\\nCardiac\\nDetection.\\n●\\nThe\\nprimary\\ngoal\\nwas\\ndeveloping\\nDeep\\nLearning\\narchitectures\\nfor\\nmedical\\nimage\\nanalysis,\\nencompassing\\nthe\\nfollowing\\nsteps:\\nacquiring\\nthe\\ndataset,')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Google PaLM Model"
      ],
      "metadata": {
        "id": "1_Op8uloNBLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GooglePalm(model_name='models/text-bison-001',\n",
        "                 temperature=0.1)"
      ],
      "metadata": {
        "id": "ImQtgDPEM7VU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "hxy3BGvINFw2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Prompts"
      ],
      "metadata": {
        "id": "9Wsh2YyDOKon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template  = \"\"\"\n",
        "Use the following piece of context to answer the question. Please provide a detailed response for each of the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in English\"\"\""
      ],
      "metadata": {
        "id": "_6GQ9rdDOILQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template = prompt_template,\n",
        "                        input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "i8p1DpI4OO5A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q/A"
      ],
      "metadata": {
        "id": "JLkj4WW7OYuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which univercity\""
      ],
      "metadata": {
        "id": "YKrnBADkOUyW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "nS6BBOhCOXEc",
        "outputId": "785a6b84-fb38-4465-cddf-f8a0eca658b4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aston University'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "iAtnAeXdOisZ",
        "outputId": "384124b3-798e-47aa-8f5b-832e1a077168"
      },
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt: projects end to end\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The candidate has worked on the following end-to-end projects:\n",
            "- End-to-End-MLOps | Stroke Disease\n",
            "- AI In Radiology\n",
            "- RSNA-Pneumonia-Detection\n",
            "Input Prompt: exit\n",
            "Exiting\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9gFLK2yOm_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}